\section{Diskussion}
Im Rahmen dieser Arbeit wurde ein System zum Klassifizieren von Sprachaufnahmen implementiert. Ein hybrides Modell aus CNN und RNN besass die höchste Genauigkeit. Das Gewinner CRNN Modell erreicht beim Librivox Testset, das von den Trainingsdaten unabhängig ist, eine Genauigkeit von 94.1\%. Das CNN Modell folgt in Genauigkeit an zweiter Stelle mit 88.1\% beim Librivox Testset. Mit 56.8\% besitzt das MobileNet Modell die mit Abstand geringste Genauigkeit im Vergleich zu den anderen beiden Modellen. Die hohe Genauigkeit der beiden Modell CRNN und CNN bestätigt, dass tiefe neuronale Netze eine angemessene Methode für Sprachidentifikation sind.
\\ \\
Der Vergleich der Resultate mit anderen Arbeiten ist nur mit bedingt möglich. Der wichtigste Unterschied ist, dass andere Daten verwendet wurden. Weil die Genauigkeit des Modells stark von der Menge und der Stichprobenverzerrung der Daten abhängt, können dementsprechend andere Resultate entstehen. Glücklicherweise sind verschiedene Arbeiten zu automatischer Sprachidentifikation im Internet frei erhältlich. Das Problem ist in der Informatik recht jung. Erst in den 70'er Jahren wurde man für das Weiterleiten von Telefonanrufen auf das Thema aufmerksam. Eine lange Zeit galt, dass die besten Systeme diejenigen waren, die die fortgeschrittensten sprachlichen Merkmale berechneten. Der Nachteil an diesen Systemen ist, dass die Erweiterung auf andere Sprachen mit einem grossem Aufwand verbunden ist. \parencite{history}
\\
Erst mit dem Aufkommen von Deep Learning als konkurrenzfähige Methode im letzten Jahrzehnt kamen wieder Systeme mit weniger Preprocessing auf \parencite{chollet}. 2009 erreichte Grégoire Montavon mit Deep Learning für 3 Sprachen auf dem Voxforge Datenset eine Genauigkeit von $80.1\%$ \parencite{montavon}. Das Voxforge Datenset hat sich seither verändert. 2016 wurde mit einem sehr ähnlichen Modell wie das CNN Modell dieser Arbeit Genauigkeiten von 93\% und 85\% für 2 Sprachen respektive 4 Sprachen gemessen \parencite{iLID}. Die Autoren haben ihre Modelle mit dem Montavon Modell verglichen und gezeigt, dass sie es deutlich übertreffen. Aus diesem Grund kann angenommen werden, dass die hier vorgestellten Modelle, das Montavon-System ebenfalls deutlich übertreffen. Hrayr et al. \parencite{yerevann} stellen im selben Jahr ein CRNN Modell für einen Wettbewerb vor, dass zwischen 176 Sprachen mit 99.67\% Genauigkeit differenzieren kann. Ein weiteres CRNN Modell von Bartz et al. \parencite{crnn} erreicht 2017 96\% bei der Unterscheidung von vier Sprachen.
\\
Die in dieser Arbeit vorgestellten System leisten vergleichbare Genaugigkeiten wie die genannten Modelle. 94.1\% ist eine überraschend hohe Genauigkeit, wenn man berücksichtigt, dass das Youtube Datenset nicht fehlerfrei ist. Das Datenset wurde bekanntlich nicht manuell gesäubert. Es können dementsprechend Aufnahmen ohne Sprache oder mit fremdsprachigen Interviews enthalten sein. Der Anteil von fehlerhaften Aufnahmen ist jedoch wahrscheinlich klein. 
80\% Genauigkeit beim Librivox Datenset ist erwartbar. Das verwendete Modell ist für die Youtube und Voxforge Daten optimiert und nicht für Hörbücher. Die Librivox Daten haben zum Beispiel unterschiedliche Hintergrundgeräusche und Aufnahmeprotokolle. 80\% bleibt in jedem Fall ein relativ hohe Genauigkeit. Ein Zufallsgenerator würde im Vergleich nur 33\% erreichen. Es kann also aus dem Resultat abgelesen werden, dass das Modell angemessen generalisiert. 
\\
Ausserdem ist es möglich die Resultate auf eine weitere interessante Weise zu interpretieren. In der Auswertung wird erwähnt, dass das CRNN Modell beim YouTube Testset Deutsch und Englisch bzw. Französisch und Englisch relativ oft verwechselt. Es ist möglich, dass mit den Sprachwissenschaft zu erklären. Englisch und Deutsch sind beides germanische Sprachen und das Französische und das Englische teilen einen grossen Wortschatz \parencite{germanic}\parencite{english}. Die Ähnlichkeiten sind also real. Bei dieser Hypothese muss man aber sehr vorsichtig sein. Dagegen spricht unter anderem, dass sich die Verwechslungen beim Librivox Datenset nicht deutlich zeigen.
\\ \\
Bis jetzt wurde das Modell nur an öffentlich verfügbaren Daten ausgewertet wo klar war welche Sprache gesprochen wurde. In den meisten praxisrelevanten Anwendungsfällen soll das Modell aber helfen, die Sprache von unbekannten Aufnahmen zu erkennen. Dafür wurde ein Interface programmiert, mit der Benutzer ihre eigene Sprache analysieren können. Das Interface ist in Form einer Webseite auf den Schulservern frei verfügbar. Die meisten Mobilgeräte und Computer sind kompatibel. Abbildung \ref{img:interface} zeigt die Seite nach dem Hochladen einer zufälligen Aufnahme.
\begin{figure}[hbt]
	\centering
		\includegraphics[width=0.6\textwidth]{assets/interface.png}
	\caption{Interface Schnappschuss}
	\label{img:interface}
\end{figure}
Die Geräuschumgebung bei Smartphone Aufnahmen unterscheidet sich dabei von Voxforge und YouTube. Die Genauigkeit die dort erreicht wird, ist eine weitere Möglichkeit die Generalisierbarkeit des Modells zu prüfen. Aus Neugier habe ich ein Datenset aus ca. 70 Aufnahmen von Verwandten und Freunden zusammengestellt. Es wurde darauf geachtet, dass die Sprache deutlich verständlich ist. Die Resultate daraus sind wegen der kleinen Anzahl und Varianz kaum signifikant. Das Programm entdeckt in 70\% der Fälle die richtige Sprache. 
Eine Programm mit Genauigkeit 70\% ist in mancher Hinsicht kein guter Klassifizierer. Kommerziell einsetzbar ist das System damit sicherlich nicht. Für ein Callcenter würde das bedeuten, dass fast jeder dritte Anruf dem falschen Mitarbeiter zugewiesen wird. Das ist aus Sicht der Kunden katastrophal. Also stellt sich die Frage woher der Fehler kommt, bzw. was man noch verbessern kann.
\\ \\
An erster Stelle stehen die Daten ein Problem. Die Trainingsdaten sind nicht repräsentativ für alle möglichen Daten, unter anderem Handy-Aufnahmen. Um die Stichprobenverzerrung weiter zu minimieren muss man mehr Daten beschaffen. Die Daten sollten so vielfältig wie möglich sein. Es ist nutzlos unendlich viele Daten zu besitzen, wenn sie sich alle sehr ähnlich sind. Einzelne relevante Merkmale sind unter anderem das Geschlecht, die Stimmhöhe, Hintergrundgeräusche, die Qualität der Aufnahme und die Geschwindigkeit. Je mehr Kombination es gibt desto besser. 
\\
Was macht man aber, wenn man nur eine begrenzte Menge an Daten zur Verfügung hat? In diesem Fall gibt es eine Methode namens \textit{Data Augmentation}. Die Lösung ist, aus den vorhanden Daten weitere Daten zu erschaffen. Manche Merkmale lassen sich nämlich automatisch anpassen. Beispielsweise kann der Computer ohne Problem die Geschwindigkeit und die Qualität der Aufnahme adaptieren. Der Computer passt bei Data Augmentation in jedem Durchlauf zufällig die Merkmale der einzelnen Aufnahmen an. Im Endeffekt wird dem Netzwerk also nie genau die gleiche Aufnahme gezeigt. Das verhindert das das Netzwerk sich an eine endliche Menge von Daten überanpasst. Zudem wird die Stichprobenverzerrung dadurch minimiert. In der Regel hat Data Augmentation immer einen positiven Effekt. Darum ist Data Augmentation  eine mächtige und weit verbreitete Methode.  In dieser Arbeit wurde primär keine Data Augmentation angewendet, weil für diese Arbeit die Annahme getroffen wurde, dass genug Daten vorhanden sind. Es stellt sich aber im Nachhinein heraus, dass Data Augmentation immer besser ist, als dem Netzwerk die gleichen Daten mehrmals zu zeigen. 
\\
An zweiter Stelle sind Mängel beim Training. Manchmal sinkt die Genauigkeit abrupt, um im nächsten Durchlauf wieder zu steigen. Die genauen Daten dazu sind online verfügbar. Die Schwankungen können nur daran liegen, dass das Validationset und das Testset zu klein sind. Das führt dazu, dass sie leicht beeinflussbar sind. Eine Erhöhung auf 25\% der gesamten Daten, ist für beide zu empfehlen. Andere Hyperparamter oder ganz andere Modelle könnten ebenfalls besser sein. Allerdings wird für viele Experimente ein leistungsstarker Computer benötigt. Mehr Leistung ist immer hilfreich.
\\
Noch mehr Verbesserungspotenzial liegt wahrscheinlich im Verfahren \textit{Learning from Between-class Examples for Deep Sound Recognition}(2017)\parencite{between}. Yuji et al. stellen eine vielversprechende Methode vor, die Genauigkeit in jedem ihrer Versuche verbessert. Die Idee ist es, Aufnahmen von verschiedenen Klassen zu kombinieren um eine Mischaufnahme zu erfinden deren Ziel ebenfalls zwischen den Klassen liegt. Vereinfacht angewendet auf Sprachidentifikation, könnte das bedeuten, dass eine Deutsche und eine Französische Aufnahme gleichzeitig abgespielt werden, um zu einer Aufnahme kombiniert zu werden. In dieser Aufnahme würden also zwei Sprecher parallel sprechen. Das Ziel der Aufnahme wäre ebenfalls ein Mix z.B $\begin{bmatrix}0.5 & 0 & 0.5 \end{bmatrix}$. Das schöne am Verfahren ist, dass es einfach zu verstehen ist und doch sehr wirksam. Leider reichte die Zeit nicht aus, um die Methode zu implementieren.
\\ \\ 
Zusammenfassend lässt sich sagen, dass immer noch Potential nach oben bleibt. Nebst dem Ergebnis ist immer auch der Weg das Ziel. Ich habe enorm viel gelernt und Erfahrung über Deep Learning gesammelt. Auf der einen Seite zeigt das Projekt wie erstaunlich einfach künstliche Intelligenz in der Praxis ist. Ohne jahrelanges Studium ist man in der Lage, das Meiste zu verstehen und zu implementieren. Nichts daran ist magisch. In vielerlei Hinsicht ist Deep Learning momentan keine absolute Wissenschaft mit wahr und falsch. Ein grosser Teil der Optimierung besteht aus heuristischen Experimenten. Bei Hyperparametern gibt es noch grossen Forschungsbedarf. Wenn einmal klar ist wie diese gesetzt werden müssen, wird Deep Learning wirklich massentauglich sein. In Zukunft wird es wohl einfachere Inferfaces geben, denen man nur das Problem angeben muss und der Rest automatisch erledigt wird. Auf der anderen Seite ist Deep Learning mathematisch ein kompliziertes Gebiet. Die Grundlagen können in Zukunft nur noch anspruchsvoller werden. Um wirklich mitforschen zu wollen, sind Mathematikkentnisse auf Universitätsstufe unabdingbar. Lineare Algebra ist hier extrem nützlich. Ein ebenfalls nicht zu unterschätzender Teil war das Sammeln und Vorbereiten der Daten. Die Vorstellung man könnte sofort an das Experimentieren heran gehen, ist trügerisch. Wenn ein Deep Learning System von Null aus aufgebaut wird, investiert man mindestens so viel Zeit für die Vorbereitung der Daten, wie beim Experimentieren. Die wichtige Frage ist wie lange Deep Learning noch so innovativ sein kann. Wird es in ein paar Jahren überhaupt noch relevant sein? Der Nutzen wird auf jedem Fall bleiben, während die mediale Aufmerksamkeit wahrscheinlich sinken wird.
\\ \\
Vielen Dank für Ihre Aufmerksamkeit und das Sie mir auf meiner (Gedanken-)Reise durch den KI-Nebel gefolgt sind. Wenn ich Ihnen beim Lesen ein wenig Wissen zum Mitnehmen vermitteln konnte, habe ich das Ziele meiner Arbeit erreicht. 
\\ \\
Herzlichst, \\
Joel Andre
