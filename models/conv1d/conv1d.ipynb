{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Conv1d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import models, layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from kapre.time_frequency import Melspectrogram\n",
    "from kapre.utils import Normalization2D\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import DataFeed\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='../preprocessing/preprocessed_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = DataFeed.Dataset.create(data_path, ['train/voxforge', 'train/youtube'], num=50000, use_premade=True)\n",
    "val_data, val_labels = DataFeed.Dataset.create(data_path, ['val/youtube', 'val/voxforge', 'test/librivox'], num=-1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = DataFeed.DataGenerator(data_path, ['train/voxforge', 'train/youtube', 'train/librivox'], num=-1, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=5),\n",
    "             keras.callbacks.ModelCheckpoint('conv1d.h5', monitor='val_loss', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Reshape((80000, 1), input_shape=(1, 80000)))\n",
    "\n",
    "model.add(layers.Conv1D(16, 6, strides=2, activation='relu', padding='same'))\n",
    "model.add(layers.MaxPooling1D((4)))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.Conv1D(32, 6, strides=2, activation='relu', padding='same'))\n",
    "model.add(layers.MaxPooling1D((4)))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.Conv1D(32, 6, activation='relu', padding='same'))\n",
    "model.add(layers.MaxPooling1D((4)))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.Conv1D(64, 6, activation='relu', padding='same'))\n",
    "model.add(layers.MaxPooling1D((4)))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.Conv1D(64, 6, activation='relu', padding='same'))\n",
    "model.add(layers.MaxPooling1D((4)))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.Conv1D(64, 6, activation='relu', padding='same'))\n",
    "model.add(layers.MaxPooling1D((4)))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Rmsprop',\n",
    "              metrics=['accuracy'],\n",
    "              loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 11080 samples\n",
      "Epoch 1/13\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 0.7010 - acc: 0.7000 - val_loss: 1.8989 - val_acc: 0.4961\n",
      "Epoch 2/13\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 0.4232 - acc: 0.8324 - val_loss: 0.8964 - val_acc: 0.6613\n",
      "Epoch 3/13\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 0.3395 - acc: 0.8687 - val_loss: 1.1722 - val_acc: 0.6835\n",
      "Epoch 4/13\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 0.2879 - acc: 0.8891 - val_loss: 0.7409 - val_acc: 0.6881\n",
      "Epoch 5/13\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 0.2564 - acc: 0.9022 - val_loss: 0.9676 - val_acc: 0.7282\n",
      "Epoch 6/13\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 0.2381 - acc: 0.9102 - val_loss: 0.7037 - val_acc: 0.7125\n",
      "Epoch 7/13\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 0.2190 - acc: 0.9183 - val_loss: 1.3154 - val_acc: 0.6787\n",
      "Epoch 8/13\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 0.2053 - acc: 0.9239 - val_loss: 0.6816 - val_acc: 0.7487\n",
      "Epoch 9/13\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 0.1922 - acc: 0.9285 - val_loss: 2.8823 - val_acc: 0.6262\n",
      "Epoch 10/13\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 0.1752 - acc: 0.9345 - val_loss: 0.4000 - val_acc: 0.8543\n",
      "Epoch 11/13\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 0.1689 - acc: 0.9377 - val_loss: 1.2773 - val_acc: 0.6840\n",
      "Epoch 12/13\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 0.1561 - acc: 0.9417 - val_loss: 1.0993 - val_acc: 0.6977\n",
      "Epoch 13/13\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 0.1511 - acc: 0.9446 - val_loss: 0.8120 - val_acc: 0.7358\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=train_data,\n",
    "                        y=train_labels,\n",
    "                        batch_size=32, \n",
    "                        epochs=13,\n",
    "                        verbose=1,\n",
    "                        validation_data=(val_data, val_labels), \n",
    "                        shuffle=True,\n",
    "                        callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 8s 753us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.3471113031387329, 0.8693], ['loss', 'acc'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = keras.models.load_model('conv1d.h5', custom_objects={'Melspectrogram':Melspectrogram, \n",
    "                                                                         'Normalization2D': Normalization2D})\n",
    "test_data, test_labels = DataFeed.Dataset.create(data_path, ['test/voxforge', 'test/youtube'], num=-1, shuffle=True)\n",
    "best_model.evaluate(x=test_data,\n",
    "                    y=test_labels), best_model.metrics_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
